{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Representaciones Inmersas (word-embeddings) Utilizando Distintos Modelos</center></h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Cálculo numérico y gestión de datos\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Módulos SMS\n",
    "import sms\n",
    "\n",
    "# Módulos para procesamiento de texto\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "# Módulos para deep learning\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense, Flatten,Dropout\n",
    "from keras.optimizers import Adam,SGD\n",
    "from keras.regularizers import l1,l2,l1_l2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualización de la Base de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  clase  numero\n",
      "0   ds0   577.0\n",
      "1   ds1   335.0\n",
      "2   ds2   473.0\n",
      "3   ds3   540.0\n",
      "4   ds4  1061.0\n",
      "5   ds5  2624.0\n",
      "6   dp0  2671.0\n",
      "7   dp1  2939.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmAAAAHsCAYAAAB18RHuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data,dt=sms.view_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga datos de entrenamiento y prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se cargó el archivo en lemas.DataFrame con 11361 lemas\n",
      "Conjunto de entrenamiento con 3550 instancias.\n",
      "Conjunto de prueba con 1522 instancias.\n"
     ]
    }
   ],
   "source": [
    "(x_train,y_train),(x_test,y_test),p_train,edf=sms.load_data(lematize=True,lem='FILE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## edf (Embeddings Data Frame) contiene el diccionario de palabras y word-embeddings:\n",
    " * KRS: Keras embeddings\n",
    " * W2V: Word2Vec embeddings\n",
    " * FST: FastText embeddings\n",
    " \n",
    "<h2><center><font color='red'>Es aqui donde hay que guardar cada vector (embedding) de cada palabra</font></center></h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Palabra</th>\n",
       "      <th>Lema</th>\n",
       "      <th>KRS</th>\n",
       "      <th>W2V</th>\n",
       "      <th>FST</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>emocional</td>\n",
       "      <td>emocional</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sumamente</td>\n",
       "      <td>sumamente</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>inestable</td>\n",
       "      <td>inestable</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>debido</td>\n",
       "      <td>debido</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>estrés</td>\n",
       "      <td>estrés</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>supone</td>\n",
       "      <td>suponer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>falta</td>\n",
       "      <td>falta</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>contacto</td>\n",
       "      <td>contacto</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hijos</td>\n",
       "      <td>hijo</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>impotencia</td>\n",
       "      <td>impotencia</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>siento</td>\n",
       "      <td>sentar</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>constatar</td>\n",
       "      <td>constatar</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>especialistas</td>\n",
       "      <td>especialista</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>seguido</td>\n",
       "      <td>seguido</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>caso</td>\n",
       "      <td>caso</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>querido</td>\n",
       "      <td>querido</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>sabido</td>\n",
       "      <td>saber</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ver</td>\n",
       "      <td>ver</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>manipulación</td>\n",
       "      <td>manipulación</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ejerce</td>\n",
       "      <td>ejercer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>padre</td>\n",
       "      <td>padre</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>preocupación</td>\n",
       "      <td>preocupación</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>análisis</td>\n",
       "      <td>análisis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>sangre</td>\n",
       "      <td>sangre</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>altas</td>\n",
       "      <td>alta</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>hice</td>\n",
       "      <td>hacer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>repetición</td>\n",
       "      <td>repetición</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>urgente</td>\n",
       "      <td>urgente</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>analítica</td>\n",
       "      <td>analítico</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>valores</td>\n",
       "      <td>valor</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12309</th>\n",
       "      <td>practiqué</td>\n",
       "      <td>practicar</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12310</th>\n",
       "      <td>oculté</td>\n",
       "      <td>ocultar</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12311</th>\n",
       "      <td>cargarlo</td>\n",
       "      <td>cargar</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12312</th>\n",
       "      <td>enoja</td>\n",
       "      <td>enojar</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12314</th>\n",
       "      <td>rayar</td>\n",
       "      <td>rayar</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12315</th>\n",
       "      <td>parrafada</td>\n",
       "      <td>parrafada</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12317</th>\n",
       "      <td>gordos</td>\n",
       "      <td>gordo</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12318</th>\n",
       "      <td>anónimos</td>\n",
       "      <td>anónimo</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12319</th>\n",
       "      <td>ajetreados</td>\n",
       "      <td>ajetreado</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12320</th>\n",
       "      <td>agradecen</td>\n",
       "      <td>agradecer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12321</th>\n",
       "      <td>bases</td>\n",
       "      <td>base</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12322</th>\n",
       "      <td>recargando</td>\n",
       "      <td>recargar</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12323</th>\n",
       "      <td>mencionado</td>\n",
       "      <td>mencionado</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12324</th>\n",
       "      <td>diseño</td>\n",
       "      <td>diseño</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12325</th>\n",
       "      <td>comprensible</td>\n",
       "      <td>comprensible</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12326</th>\n",
       "      <td>fuer</td>\n",
       "      <td>fuer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12327</th>\n",
       "      <td>finca</td>\n",
       "      <td>finca</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12329</th>\n",
       "      <td>marginada</td>\n",
       "      <td>marginado</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12330</th>\n",
       "      <td>desahogar</td>\n",
       "      <td>desahogar</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12331</th>\n",
       "      <td>felicite</td>\n",
       "      <td>felicitar</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12332</th>\n",
       "      <td>estafo</td>\n",
       "      <td>estafar</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12333</th>\n",
       "      <td>juntaba</td>\n",
       "      <td>juntar</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12334</th>\n",
       "      <td>juntarme</td>\n",
       "      <td>juntar</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12336</th>\n",
       "      <td>relajadamente</td>\n",
       "      <td>relajadamente</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12337</th>\n",
       "      <td>ininterrumpidamente</td>\n",
       "      <td>ininterrumpidamente</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12338</th>\n",
       "      <td>reanudar</td>\n",
       "      <td>reanudar</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12339</th>\n",
       "      <td>ocurría</td>\n",
       "      <td>ocurrir</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12340</th>\n",
       "      <td>vertiginosa</td>\n",
       "      <td>vertiginoso</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12341</th>\n",
       "      <td>advirtió</td>\n",
       "      <td>advertir</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12342</th>\n",
       "      <td>bajaba</td>\n",
       "      <td>bajar</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11361 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Palabra                 Lema  KRS  W2V  FST\n",
       "0                emocional            emocional    0    0    0\n",
       "1                sumamente            sumamente    0    0    0\n",
       "2                inestable            inestable    0    0    0\n",
       "3                   debido               debido    0    0    0\n",
       "4                   estrés              estrés    0    0    0\n",
       "5                   supone              suponer    0    0    0\n",
       "6                    falta                falta    0    0    0\n",
       "7                 contacto             contacto    0    0    0\n",
       "8                    hijos                 hijo    0    0    0\n",
       "9               impotencia           impotencia    0    0    0\n",
       "10                  siento               sentar    0    0    0\n",
       "11               constatar            constatar    0    0    0\n",
       "12           especialistas         especialista    0    0    0\n",
       "13                 seguido              seguido    0    0    0\n",
       "14                    caso                 caso    0    0    0\n",
       "15                 querido              querido    0    0    0\n",
       "16                  sabido                saber    0    0    0\n",
       "17                     ver                  ver    0    0    0\n",
       "18            manipulación        manipulación    0    0    0\n",
       "19                  ejerce              ejercer    0    0    0\n",
       "20                   padre                padre    0    0    0\n",
       "21            preocupación        preocupación    0    0    0\n",
       "22                análisis            análisis    0    0    0\n",
       "23                  sangre               sangre    0    0    0\n",
       "25                   altas                 alta    0    0    0\n",
       "26                    hice                hacer    0    0    0\n",
       "27              repetición          repetición    0    0    0\n",
       "28                 urgente              urgente    0    0    0\n",
       "29               analítica           analítico    0    0    0\n",
       "30                 valores                valor    0    0    0\n",
       "...                    ...                  ...  ...  ...  ...\n",
       "12309            practiqué            practicar    0    0    0\n",
       "12310               oculté              ocultar    0    0    0\n",
       "12311             cargarlo               cargar    0    0    0\n",
       "12312                enoja               enojar    0    0    0\n",
       "12314                rayar                rayar    0    0    0\n",
       "12315            parrafada            parrafada    0    0    0\n",
       "12317               gordos                gordo    0    0    0\n",
       "12318             anónimos             anónimo    0    0    0\n",
       "12319           ajetreados            ajetreado    0    0    0\n",
       "12320            agradecen            agradecer    0    0    0\n",
       "12321                bases                 base    0    0    0\n",
       "12322           recargando             recargar    0    0    0\n",
       "12323           mencionado           mencionado    0    0    0\n",
       "12324               diseño              diseño    0    0    0\n",
       "12325         comprensible         comprensible    0    0    0\n",
       "12326                 fuer                 fuer    0    0    0\n",
       "12327                finca                finca    0    0    0\n",
       "12329            marginada            marginado    0    0    0\n",
       "12330            desahogar            desahogar    0    0    0\n",
       "12331             felicite            felicitar    0    0    0\n",
       "12332               estafo              estafar    0    0    0\n",
       "12333              juntaba               juntar    0    0    0\n",
       "12334             juntarme               juntar    0    0    0\n",
       "12336        relajadamente        relajadamente    0    0    0\n",
       "12337  ininterrumpidamente  ininterrumpidamente    0    0    0\n",
       "12338             reanudar             reanudar    0    0    0\n",
       "12339              ocurría              ocurrir    0    0    0\n",
       "12340          vertiginosa          vertiginoso    0    0    0\n",
       "12341             advirtió             advertir    0    0    0\n",
       "12342               bajaba                bajar    0    0    0\n",
       "\n",
       "[11361 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edf #data frame donde hay que guardar los embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificación de algunos datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape de x_train:\n",
      "(3550,) \n",
      "\n",
      "x_train[0]:\n",
      "parece ser llevo varios días mejor empezado hacer tercera edad terapia ocupacional escribo cada cuatro cinco días sigo deprax medicamento entraba ansias comida ahora apetito normal \n",
      "\n",
      "y_train[0]:\n",
      "[0. 0. 0. 0. 1. 0. 0. 1.] \n",
      "\n",
      "x_test[0]:\n",
      "días mal desanimada dicho tío cáncer hígado afectado veces da mucha ansiedad metido atracones trozos tarta grande chocolate día siguiente mal cabeza dijo vomitar sentía gorda familiares hice si creo hecho paraba pensar día sentirme culpable sufro pesadillas levanto encharcada sudor ex pareja pegaba tiro día agobiada \n",
      "\n",
      "y_test[0]:\n",
      "[0. 1. 0. 0. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "print('Shape de x_train:')\n",
    "print(x_train.shape,'\\n')\n",
    "\n",
    "print('x_train[0]:')\n",
    "print(x_train[0],'\\n')\n",
    "print('y_train[0]:')\n",
    "print(y_train[0],'\\n')\n",
    "print('x_test[0]:')\n",
    "print(x_test[0],'\\n')\n",
    "print('y_test[0]:')\n",
    "print(y_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo KRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parametrización y arquitectura del modelo KRS\n",
    "# Entradas:\n",
    "#  * Datos de entrenamiento: frases de texto\n",
    "#  * Datos de prueba\n",
    "#  * Proporción del vocabulario a utilizar\n",
    "#  * maxima longitud de frases (bool)\n",
    "# Salidas:\n",
    "#  * Datos de entrenamiento y prueba como matrices de números enteros\n",
    "#  * Tokenizador\n",
    "#  * Tamaño del vocabulario\n",
    "#  * Longitud de la secuencia numérica (frase)\n",
    "def keras_embedding(x_train,x_test,voc_prop=1,maxl=True):\n",
    "    # set vocabulary size\n",
    "    train_vocab_size = len(set(word_tokenize(\" \".join(x_train))))\n",
    "    vocab_size = int(voc_prop*train_vocab_size)\n",
    "    print(f\"Hay {train_vocab_size} palabras únicas en todo el set de entrenamiento.\")\n",
    "    print(f\"Conservando {voc_prop*100}% ({vocab_size}) como tamaño del vocabulario.\")\n",
    "\n",
    "    #tokenize train_ and test_ texts and compute integer codes per word\n",
    "    tok = Tokenizer(num_words=vocab_size)\n",
    "    tok.fit_on_texts(x_train)\n",
    "    X_train = tok.texts_to_sequences(x_train)\n",
    "    X_test = tok.texts_to_sequences(x_test)\n",
    "    # Zero padding\n",
    "    lengths = np.array([len(x) for x in X_train])\n",
    "\n",
    "    # Sentence size (input)\n",
    "    if not maxl:\n",
    "        seque_leng = int(lengths.mean() + 2 * lengths.std())\n",
    "        mt = \"(mu + 2*sigma)\"\n",
    "    else:\n",
    "        seque_leng = max(lengths)\n",
    "        mt = \"(max length)\"\n",
    "    print(f\"Longitud de las frases = {seque_leng} palabras {mt}.\")\n",
    "\n",
    "    X_train = sequence.pad_sequences(X_train, maxlen=seque_leng, padding='pre')\n",
    "    X_test = sequence.pad_sequences(X_test, maxlen=seque_leng, padding='pre')\n",
    "\n",
    "    return X_train, X_test,tok,vocab_size,seque_leng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualiza los códigos para X_train[0] <font color='red'>OJO! X_train != x_train</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hay 10286 palabras únicas en todo el set de entrenamiento.\n",
      "Conservando 100% (10286) como tamaño del vocabulario.\n",
      "Longitud de las frases = 79 palabras (max length).\n",
      "\n",
      "Shape de X_train:\n",
      "(3550, 79) \n",
      "\n",
      "\n",
      " X_train[0]:=\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0   94   54   51\n",
      "  407    6   10  226    8 2738  893  531 3515 1075   49  292  507    6\n",
      "   21  993  894 5087 3516   95   34  366   87] \n",
      "\n",
      "parece : 94 , ser : 54 , llevo : 51 , varios : 407 , días : 6 , mejor : 10 , empezado : 226 , hacer : 8 , tercera : 2738 , edad : 893 , terapia : 531 , ocupacional : 3515 , escribo : 1075 , cada : 49 , cuatro : 292 , cinco : 507 , días : 6 , sigo : 21 , deprax : 993 , medicamento : 894 , entraba : 5087 , ansias : 3516 , comida : 95 , ahora : 34 , apetito : 366 , normal : 87 , "
     ]
    }
   ],
   "source": [
    "# X_train != x_train\n",
    "# X_train solo se usa para el modelo KRS\n",
    "X_train,X_test,tokenizer,vs,sl = keras_embedding(x_train,x_test,voc_prop=1)\n",
    "print('\\nShape de X_train:')\n",
    "print(X_train.shape,'\\n')\n",
    "\n",
    "print('\\n X_train[0]:=\\n',X_train[0],'\\n')\n",
    "word_dict=tokenizer.word_index\n",
    "#Check codes\n",
    "for code in X_train[0]:\n",
    "    if code != 0:\n",
    "        print(list(word_dict.keys())[list(word_dict.values()).index(code)],':',code,',',end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arquitectura KRS <font color='red'>(correr antes de un nuevo entrenamiento)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_25 (Embedding)     (None, 79, 100)           1028600   \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 79, 32)            9632      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 39, 32)            0         \n",
      "_________________________________________________________________\n",
      "lstm_25 (LSTM)               (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 8)                 264       \n",
      "=================================================================\n",
      "Total params: 1,046,816\n",
      "Trainable params: 1,046,816\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Size of word embeddings\n",
    "vec_dim=100\n",
    "\n",
    "# KRS Model vs y sl calculados arriba\n",
    "KRS = Sequential()\n",
    "KRS.add(Embedding(input_dim=vs, output_dim=vec_dim, input_length=sl))\n",
    "KRS.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "# Param = embedding_vect_len * kernel_size * #filters + #filters = 3104\n",
    "KRS.add(MaxPooling1D(pool_size=2))\n",
    "KRS.add(LSTM(units=32, dropout=0.25, recurrent_dropout=0.25))\n",
    "KRS.add(Dense(units=8, activation='sigmoid'))\n",
    "#KRS.add(Dense(units=8,kernel_regularizer=l2(5E-3), activation='sigmoid'))\n",
    "\n",
    "# compile network\n",
    "KRS.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(KRS.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento del modelo KRS\n",
    "<p>\n",
    "<font color='red'>El modelo se guarda en un archivo que se puede recuperar para extraer los embeddings</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3550 samples, validate on 1522 samples\n",
      "Epoch 1/500\n",
      " - 9s - loss: 0.4881 - acc: 0.7538 - val_loss: 0.4648 - val_acc: 0.7678\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.46482, saving model to KRS_weights_best_3rd.hdf5\n",
      "Epoch 2/500\n",
      " - 8s - loss: 0.4247 - acc: 0.8183 - val_loss: 0.4458 - val_acc: 0.7997\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.46482 to 0.44585, saving model to KRS_weights_best_3rd.hdf5\n",
      "Epoch 3/500\n",
      " - 7s - loss: 0.3121 - acc: 0.8813 - val_loss: 0.4909 - val_acc: 0.7962\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.44585\n",
      "Epoch 4/500\n",
      " - 8s - loss: 0.2411 - acc: 0.9088 - val_loss: 0.5593 - val_acc: 0.7974\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.44585\n",
      "Epoch 5/500\n",
      " - 7s - loss: 0.2034 - acc: 0.9199 - val_loss: 0.6330 - val_acc: 0.7994\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.44585\n",
      "Epoch 6/500\n",
      " - 7s - loss: 0.1836 - acc: 0.9256 - val_loss: 0.6789 - val_acc: 0.7946\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.44585\n",
      "Epoch 7/500\n",
      " - 7s - loss: 0.1700 - acc: 0.9292 - val_loss: 0.7109 - val_acc: 0.7878\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.44585\n",
      "Epoch 8/500\n",
      " - 7s - loss: 0.1512 - acc: 0.9400 - val_loss: 0.7635 - val_acc: 0.7852\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.44585\n",
      "Epoch 9/500\n",
      " - 7s - loss: 0.1342 - acc: 0.9474 - val_loss: 0.8036 - val_acc: 0.7777\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.44585\n",
      "Epoch 10/500\n",
      " - 7s - loss: 0.1178 - acc: 0.9532 - val_loss: 0.8655 - val_acc: 0.7685\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.44585\n",
      "Epoch 11/500\n",
      " - 7s - loss: 0.1017 - acc: 0.9596 - val_loss: 0.8822 - val_acc: 0.7673\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.44585\n",
      "Epoch 12/500\n",
      " - 7s - loss: 0.0886 - acc: 0.9651 - val_loss: 0.9240 - val_acc: 0.7679\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.44585\n",
      "Epoch 13/500\n",
      " - 7s - loss: 0.0771 - acc: 0.9726 - val_loss: 0.9525 - val_acc: 0.7663\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.44585\n",
      "Epoch 14/500\n",
      " - 7s - loss: 0.0681 - acc: 0.9769 - val_loss: 0.9856 - val_acc: 0.7656\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.44585\n",
      "Epoch 15/500\n",
      " - 7s - loss: 0.0611 - acc: 0.9799 - val_loss: 1.0122 - val_acc: 0.7671\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.44585\n",
      "Epoch 16/500\n",
      " - 7s - loss: 0.0534 - acc: 0.9839 - val_loss: 1.0468 - val_acc: 0.7610\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.44585\n",
      "Epoch 17/500\n",
      " - 7s - loss: 0.0455 - acc: 0.9865 - val_loss: 1.0625 - val_acc: 0.7626\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.44585\n",
      "Epoch 18/500\n",
      " - 7s - loss: 0.0417 - acc: 0.9886 - val_loss: 1.0846 - val_acc: 0.7645\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.44585\n",
      "Epoch 19/500\n",
      " - 7s - loss: 0.0354 - acc: 0.9915 - val_loss: 1.1287 - val_acc: 0.7530\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.44585\n",
      "Epoch 20/500\n",
      " - 7s - loss: 0.0311 - acc: 0.9924 - val_loss: 1.1590 - val_acc: 0.7480\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.44585\n",
      "Epoch 21/500\n",
      " - 7s - loss: 0.0282 - acc: 0.9926 - val_loss: 1.1518 - val_acc: 0.7571\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.44585\n",
      "Epoch 22/500\n",
      " - 7s - loss: 0.0263 - acc: 0.9931 - val_loss: 1.1555 - val_acc: 0.7553\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.44585\n",
      "Epoch 23/500\n",
      " - 7s - loss: 0.0221 - acc: 0.9949 - val_loss: 1.2179 - val_acc: 0.7513\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.44585\n",
      "Epoch 24/500\n",
      " - 7s - loss: 0.0215 - acc: 0.9950 - val_loss: 1.2187 - val_acc: 0.7566\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.44585\n",
      "Epoch 25/500\n",
      " - 8s - loss: 0.0179 - acc: 0.9961 - val_loss: 1.2600 - val_acc: 0.7498\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.44585\n",
      "Epoch 26/500\n",
      " - 7s - loss: 0.0155 - acc: 0.9969 - val_loss: 1.2470 - val_acc: 0.7533\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.44585\n",
      "Epoch 27/500\n",
      " - 9s - loss: 0.0147 - acc: 0.9967 - val_loss: 1.2763 - val_acc: 0.7544\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.44585\n",
      "Epoch 28/500\n",
      " - 10s - loss: 0.0138 - acc: 0.9970 - val_loss: 1.3320 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.44585\n",
      "Epoch 29/500\n",
      " - 9s - loss: 0.0122 - acc: 0.9974 - val_loss: 1.3368 - val_acc: 0.7490\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.44585\n",
      "Epoch 30/500\n",
      " - 8s - loss: 0.0111 - acc: 0.9980 - val_loss: 1.3303 - val_acc: 0.7518\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.44585\n",
      "Epoch 31/500\n",
      " - 8s - loss: 0.0108 - acc: 0.9977 - val_loss: 1.3682 - val_acc: 0.7499\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.44585\n",
      "Epoch 32/500\n",
      " - 8s - loss: 0.0126 - acc: 0.9966 - val_loss: 1.3612 - val_acc: 0.7505\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.44585\n",
      "Epoch 33/500\n",
      " - 8s - loss: 0.0094 - acc: 0.9980 - val_loss: 1.3738 - val_acc: 0.7533\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.44585\n",
      "Epoch 34/500\n",
      " - 8s - loss: 0.0085 - acc: 0.9984 - val_loss: 1.4061 - val_acc: 0.7464\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.44585\n",
      "Epoch 35/500\n",
      " - 8s - loss: 0.0114 - acc: 0.9965 - val_loss: 1.4200 - val_acc: 0.7472\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.44585\n",
      "Epoch 36/500\n",
      " - 8s - loss: 0.0088 - acc: 0.9978 - val_loss: 1.4179 - val_acc: 0.7535\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.44585\n",
      "Epoch 37/500\n",
      " - 8s - loss: 0.0071 - acc: 0.9987 - val_loss: 1.4262 - val_acc: 0.7536\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.44585\n",
      "Epoch 38/500\n",
      " - 8s - loss: 0.0075 - acc: 0.9983 - val_loss: 1.4332 - val_acc: 0.7534\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.44585\n",
      "Epoch 39/500\n",
      " - 7s - loss: 0.0077 - acc: 0.9979 - val_loss: 1.4827 - val_acc: 0.7521\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.44585\n",
      "Epoch 40/500\n",
      " - 7s - loss: 0.0071 - acc: 0.9983 - val_loss: 1.5069 - val_acc: 0.7509\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.44585\n",
      "Epoch 41/500\n",
      " - 7s - loss: 0.0055 - acc: 0.9986 - val_loss: 1.5207 - val_acc: 0.7522\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.44585\n",
      "Epoch 42/500\n",
      " - 7s - loss: 0.0052 - acc: 0.9989 - val_loss: 1.5525 - val_acc: 0.7524\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.44585\n",
      "Epoch 43/500\n",
      " - 7s - loss: 0.0048 - acc: 0.9988 - val_loss: 1.5772 - val_acc: 0.7476\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.44585\n",
      "Epoch 44/500\n",
      " - 7s - loss: 0.0041 - acc: 0.9990 - val_loss: 1.6062 - val_acc: 0.7460\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.44585\n",
      "Epoch 45/500\n",
      " - 7s - loss: 0.0042 - acc: 0.9990 - val_loss: 1.5816 - val_acc: 0.7555\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.44585\n",
      "Epoch 46/500\n",
      " - 7s - loss: 0.0049 - acc: 0.9985 - val_loss: 1.5754 - val_acc: 0.7572\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.44585\n",
      "Epoch 47/500\n",
      " - 7s - loss: 0.0057 - acc: 0.9983 - val_loss: 1.5799 - val_acc: 0.7502\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.44585\n",
      "Epoch 48/500\n",
      " - 7s - loss: 0.0061 - acc: 0.9979 - val_loss: 1.6151 - val_acc: 0.7522\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.44585\n",
      "Epoch 49/500\n",
      " - 7s - loss: 0.0055 - acc: 0.9984 - val_loss: 1.6226 - val_acc: 0.7488\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.44585\n",
      "Epoch 50/500\n",
      " - 7s - loss: 0.0052 - acc: 0.9986 - val_loss: 1.6339 - val_acc: 0.7485\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.44585\n",
      "Epoch 51/500\n",
      " - 7s - loss: 0.0043 - acc: 0.9989 - val_loss: 1.6298 - val_acc: 0.7488\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.44585\n",
      "Epoch 52/500\n",
      " - 8s - loss: 0.0039 - acc: 0.9988 - val_loss: 1.6488 - val_acc: 0.7479\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.44585\n",
      "Epoch 53/500\n",
      " - 7s - loss: 0.0032 - acc: 0.9993 - val_loss: 1.6720 - val_acc: 0.7538\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.44585\n",
      "Epoch 54/500\n",
      " - 7s - loss: 0.0032 - acc: 0.9991 - val_loss: 1.6920 - val_acc: 0.7512\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.44585\n",
      "Epoch 55/500\n",
      " - 8s - loss: 0.0030 - acc: 0.9993 - val_loss: 1.7136 - val_acc: 0.7502\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.44585\n",
      "Epoch 56/500\n",
      " - 7s - loss: 0.0031 - acc: 0.9992 - val_loss: 1.7138 - val_acc: 0.7512\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.44585\n",
      "Epoch 57/500\n",
      " - 7s - loss: 0.0050 - acc: 0.9980 - val_loss: 1.7020 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.44585\n",
      "Epoch 58/500\n",
      " - 7s - loss: 0.0038 - acc: 0.9989 - val_loss: 1.7234 - val_acc: 0.7495\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.44585\n",
      "Epoch 59/500\n",
      " - 7s - loss: 0.0026 - acc: 0.9994 - val_loss: 1.7533 - val_acc: 0.7484\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.44585\n",
      "Epoch 60/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 8s - loss: 0.0027 - acc: 0.9994 - val_loss: 1.7598 - val_acc: 0.7488\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.44585\n",
      "Epoch 61/500\n",
      " - 7s - loss: 0.0029 - acc: 0.9992 - val_loss: 1.7396 - val_acc: 0.7528\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.44585\n",
      "Epoch 62/500\n",
      " - 8s - loss: 0.0025 - acc: 0.9993 - val_loss: 1.7417 - val_acc: 0.7472\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.44585\n",
      "Epoch 63/500\n",
      " - 7s - loss: 0.0025 - acc: 0.9994 - val_loss: 1.7828 - val_acc: 0.7498\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.44585\n",
      "Epoch 64/500\n",
      " - 7s - loss: 0.0025 - acc: 0.9993 - val_loss: 1.7623 - val_acc: 0.7499\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.44585\n",
      "Epoch 65/500\n",
      " - 7s - loss: 0.0028 - acc: 0.9995 - val_loss: 1.7865 - val_acc: 0.7501\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.44585\n",
      "Epoch 66/500\n",
      " - 7s - loss: 0.0025 - acc: 0.9993 - val_loss: 1.7509 - val_acc: 0.7558\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.44585\n",
      "Epoch 67/500\n",
      " - 8s - loss: 0.0019 - acc: 0.9995 - val_loss: 1.7787 - val_acc: 0.7532\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.44585\n",
      "Epoch 68/500\n",
      " - 7s - loss: 0.0018 - acc: 0.9996 - val_loss: 1.8295 - val_acc: 0.7503\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.44585\n",
      "Epoch 69/500\n",
      " - 8s - loss: 0.0025 - acc: 0.9993 - val_loss: 1.7704 - val_acc: 0.7517\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.44585\n",
      "Epoch 70/500\n",
      " - 7s - loss: 0.0027 - acc: 0.9988 - val_loss: 1.8438 - val_acc: 0.7474\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.44585\n",
      "Epoch 71/500\n",
      " - 8s - loss: 0.0024 - acc: 0.9993 - val_loss: 1.8650 - val_acc: 0.7473\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.44585\n",
      "Epoch 72/500\n",
      " - 8s - loss: 0.0021 - acc: 0.9995 - val_loss: 1.8960 - val_acc: 0.7437\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.44585\n",
      "Epoch 73/500\n",
      " - 8s - loss: 0.0020 - acc: 0.9994 - val_loss: 1.8868 - val_acc: 0.7455\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.44585\n",
      "Epoch 74/500\n",
      " - 8s - loss: 0.0022 - acc: 0.9993 - val_loss: 1.8987 - val_acc: 0.7447\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.44585\n",
      "Epoch 75/500\n",
      " - 7s - loss: 0.0035 - acc: 0.9987 - val_loss: 1.8734 - val_acc: 0.7438\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.44585\n",
      "Epoch 76/500\n",
      " - 7s - loss: 0.0023 - acc: 0.9994 - val_loss: 1.8851 - val_acc: 0.7459\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.44585\n",
      "Epoch 77/500\n",
      " - 7s - loss: 0.0012 - acc: 0.9996 - val_loss: 1.9045 - val_acc: 0.7460\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.44585\n",
      "Epoch 78/500\n",
      " - 7s - loss: 0.0018 - acc: 0.9996 - val_loss: 1.9124 - val_acc: 0.7433\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.44585\n",
      "Epoch 79/500\n",
      " - 7s - loss: 0.0017 - acc: 0.9994 - val_loss: 1.9102 - val_acc: 0.7459\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.44585\n",
      "Epoch 80/500\n",
      " - 8s - loss: 0.0021 - acc: 0.9994 - val_loss: 1.8661 - val_acc: 0.7563\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.44585\n",
      "Epoch 81/500\n",
      " - 7s - loss: 0.0013 - acc: 0.9995 - val_loss: 1.9059 - val_acc: 0.7493\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.44585\n",
      "Epoch 82/500\n",
      " - 7s - loss: 0.0020 - acc: 0.9993 - val_loss: 1.9282 - val_acc: 0.7463\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.44585\n",
      "Epoch 83/500\n",
      " - 7s - loss: 0.0012 - acc: 0.9997 - val_loss: 1.9475 - val_acc: 0.7451\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.44585\n",
      "Epoch 84/500\n",
      " - 7s - loss: 9.2300e-04 - acc: 0.9998 - val_loss: 1.9457 - val_acc: 0.7463\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.44585\n",
      "Epoch 85/500\n",
      " - 7s - loss: 0.0015 - acc: 0.9994 - val_loss: 1.9553 - val_acc: 0.7459\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.44585\n",
      "Epoch 86/500\n",
      " - 8s - loss: 0.0015 - acc: 0.9995 - val_loss: 1.9571 - val_acc: 0.7459\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.44585\n",
      "Epoch 87/500\n",
      " - 7s - loss: 0.0012 - acc: 0.9996 - val_loss: 1.9501 - val_acc: 0.7475\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.44585\n",
      "Epoch 88/500\n",
      " - 7s - loss: 0.0011 - acc: 0.9997 - val_loss: 1.9673 - val_acc: 0.7475\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.44585\n",
      "Epoch 89/500\n",
      " - 7s - loss: 0.0017 - acc: 0.9995 - val_loss: 2.0382 - val_acc: 0.7431\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.44585\n",
      "Epoch 90/500\n",
      " - 7s - loss: 0.0013 - acc: 0.9995 - val_loss: 1.9527 - val_acc: 0.7524\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.44585\n",
      "Epoch 91/500\n",
      " - 7s - loss: 7.6313e-04 - acc: 0.9998 - val_loss: 1.9739 - val_acc: 0.7511\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.44585\n",
      "Epoch 92/500\n",
      " - 7s - loss: 8.8482e-04 - acc: 0.9997 - val_loss: 2.0198 - val_acc: 0.7480\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.44585\n",
      "Epoch 93/500\n",
      " - 8s - loss: 0.0020 - acc: 0.9993 - val_loss: 1.9799 - val_acc: 0.7461\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.44585\n",
      "Epoch 94/500\n",
      " - 7s - loss: 0.0031 - acc: 0.9992 - val_loss: 2.0079 - val_acc: 0.7433\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.44585\n",
      "Epoch 95/500\n",
      " - 7s - loss: 0.0035 - acc: 0.9992 - val_loss: 2.0007 - val_acc: 0.7484\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.44585\n",
      "Epoch 96/500\n",
      " - 7s - loss: 8.7355e-04 - acc: 0.9998 - val_loss: 2.0105 - val_acc: 0.7468\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.44585\n",
      "Epoch 97/500\n",
      " - 7s - loss: 8.4961e-04 - acc: 0.9999 - val_loss: 2.0296 - val_acc: 0.7448\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.44585\n",
      "Epoch 98/500\n",
      " - 7s - loss: 0.0011 - acc: 0.9997 - val_loss: 2.0663 - val_acc: 0.7426\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.44585\n",
      "Epoch 99/500\n",
      " - 7s - loss: 9.6821e-04 - acc: 0.9998 - val_loss: 2.0512 - val_acc: 0.7456\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.44585\n",
      "Epoch 100/500\n",
      " - 7s - loss: 6.9531e-04 - acc: 0.9999 - val_loss: 2.0550 - val_acc: 0.7489\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.44585\n",
      "Epoch 101/500\n",
      " - 7s - loss: 0.0020 - acc: 0.9994 - val_loss: 2.0221 - val_acc: 0.7488\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.44585\n",
      "Epoch 102/500\n",
      " - 7s - loss: 9.9869e-04 - acc: 0.9997 - val_loss: 2.0660 - val_acc: 0.7434\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.44585\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3941316860>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "\n",
    "filepath=\"KRS_weights_best_3rd.hdf5\" #último archivo guardado\n",
    "epocas=500\n",
    "paciencia=100\n",
    "batch=16\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=paciencia, mode='auto') \n",
    "callbacks_list = [checkpoint, early_stop]\n",
    "\n",
    "KRS.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=batch, verbose=2, epochs=epocas, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------\n",
    "<h1><center><font color='blue'>POR HACER A PARTIR DE AQUI:</font></center></h1> \n",
    "# --------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <p>\n",
    " <ol>\n",
    "  <li>Recuperar los vectores de KRS por cada palabra y almacenarlos en <b>edf</b>:\n",
    "      <ul>\n",
    "          <li>Checar el archivo WRD_EMBED_TUT en la parte de <b>Extracción de WE</b></li>\n",
    "      </ul>\n",
    "  </li>\n",
    "  <li>Terminar de implementar el <b>Modelo Word2Vec</b>. Checar estas referencias:</li>\n",
    "      <ul>\n",
    "          <li><a href=\"https://rare-technologies.com/word2vec-tutorial/\">Tutorial W2V</a></li>\n",
    "          <li><a href=\"https://www.shanelynn.ie/word-embeddings-in-python-with-spacy-and-gensim/\">WE in Python with gensim</a></li>\n",
    "          <li><a href=\"https://radimrehurek.com/gensim/models/keyedvectors.html\">gensim models.keyedvectors – Store and query word vectors</a></li>\n",
    "      </ul>\n",
    "  <li>Implementar el <b>Modelo FastText</b></li>\n",
    "      <ul>\n",
    "          <li><a href=\"https://radimrehurek.com/gensim/models/fasttext.html#module-gensim.models.fasttext\">gensim models.fasttext – FastText model</a></li>\n",
    "          <li><a href=\"https://rare-technologies.com/fasttext-and-gensim-word-embeddings/\">Discusión FastText model</a></li>\n",
    "          <li><a href=\"https://pypi.org/project/fasttext/\">ALTERNATIVAMENTE se puede usar este Módulo Fast-Text de Python (no es gensim)</a></li>\n",
    "      </ul>\n",
    "</ol>\n",
    "</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import modules & set up logging\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Preliminary KNOWLEDGE:\n",
    "# gensim’s word2vec expects a sequence of sentences as its input. \n",
    "# Each sentence a list of words (utf8 strings): \n",
    "# FOR INSTANCE:\n",
    "# sentences = [['first', 'sentence'], ['second', 'sentence']]\n",
    "# train word2vec on the two sentences\n",
    "# model = gensim.models.Word2Vec(sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterador (ahorra memoria) sobre documentos que contienen una frase de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class MySentences(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    "    # Iterador de frases\n",
    "    # El iterador abre uno a uno los archivos que contienen cada una de las frases\n",
    "    # Por lo tanto debe haber 3550 documentos, \n",
    "    # que corresponden a las instancias en x_train\n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            for line in open(os.path.join(self.dirname, fname)):\n",
    "                yield line.split()\n",
    "# La carpeta Frases está VACÍA!! \n",
    "# Necesitamos llenarla con documentos que contengan las instancias de x_train\n",
    "# Así, la variable all_sentences contiene las instancias de entrenamiento\n",
    "all_sentences = MySentences('Frases/') # a memory-friendly iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento de Word2Vec\n",
    "<p>\n",
    "Se puede jugar con los parámetros\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(all_sentences, \n",
    "                 min_count=1,   # Ignore words that appear less than this\n",
    "                 size=100,      # Dimensionality of word embeddings\n",
    "                 workers=4,     # Number of processors (parallelisation) requires cython\n",
    "                 window=5,      # Context window for words during training\n",
    "                 iter=30)       # Number of epochs training over corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# para convertir esta libreta en un programa .py que puede ser llamado\n",
    "# NO ES FORZOSO CORRER ESTA INSTRUCCION\n",
    "!jupyter nbconvert --to script SMS_Embeddings.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
