{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Representaciones Inmersas (word-embeddings) Utilizando Distintos Modelos</center></h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo numérico y gestión de datos\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Módulos SMS\n",
    "import sms\n",
    "\n",
    "# Módulos para procesamiento de texto\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "# Módulos para deep learning\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense, Flatten,Dropout\n",
    "from keras.optimizers import Adam,SGD\n",
    "from keras.regularizers import l1,l2,l1_l2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualización de la Base de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  clase  numero\n",
      "0   ds0   577.0\n",
      "1   ds1   335.0\n",
      "2   ds2   473.0\n",
      "3   ds3   540.0\n",
      "4   ds4  1061.0\n",
      "5   ds5  2624.0\n",
      "6   dp0  2671.0\n",
      "7   dp1  2939.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data,dt=sms.view_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga datos de entrenamiento y prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se cargó el archivo en lemas.DataFrame con 11361 lemas\n",
      "Conjunto de entrenamiento con 3550 instancias.\n",
      "Conjunto de prueba con 1522 instancias.\n"
     ]
    }
   ],
   "source": [
    "(x_train,y_train),(x_test,y_test),p_train,edf=sms.load_data(lematize=True,lem='FILE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## edf (Embeddings Data Frame) contiene el diccionario de palabras y word-embeddings:\n",
    " * KRS: Keras embeddings\n",
    " * W2V: Word2Vec embeddings\n",
    " * FST: FastText embeddings\n",
    " \n",
    "<h2><center><font color='red'>Es aqui donde hay que guardar cada vector (embedding) de cada palabra</font></center></h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Palabra</th>\n",
       "      <th>Lema</th>\n",
       "      <th>KRS</th>\n",
       "      <th>W2V</th>\n",
       "      <th>FST</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>emocional</td>\n",
       "      <td>emocional</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sumamente</td>\n",
       "      <td>sumamente</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>inestable</td>\n",
       "      <td>inestable</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>debido</td>\n",
       "      <td>debido</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>estrés</td>\n",
       "      <td>estreì�s</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>supone</td>\n",
       "      <td>suponer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>falta</td>\n",
       "      <td>falta</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>contacto</td>\n",
       "      <td>contacto</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hijos</td>\n",
       "      <td>hijo</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>impotencia</td>\n",
       "      <td>impotencia</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>siento</td>\n",
       "      <td>sentar</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>constatar</td>\n",
       "      <td>constatar</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>especialistas</td>\n",
       "      <td>especialista</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>seguido</td>\n",
       "      <td>seguido</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>caso</td>\n",
       "      <td>caso</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>querido</td>\n",
       "      <td>querido</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>sabido</td>\n",
       "      <td>saber</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ver</td>\n",
       "      <td>ver</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>manipulación</td>\n",
       "      <td>manipulacioì�n</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ejerce</td>\n",
       "      <td>ejercer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>padre</td>\n",
       "      <td>padre</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>preocupación</td>\n",
       "      <td>preocupacioì�n</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>análisis</td>\n",
       "      <td>anaì�lisis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>sangre</td>\n",
       "      <td>sangre</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>altas</td>\n",
       "      <td>alta</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>hice</td>\n",
       "      <td>hacer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>repetición</td>\n",
       "      <td>repeticioì�n</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>urgente</td>\n",
       "      <td>urgente</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>analítica</td>\n",
       "      <td>analiì�tico</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>valores</td>\n",
       "      <td>valor</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12309</th>\n",
       "      <td>practiqué</td>\n",
       "      <td>practicar</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12310</th>\n",
       "      <td>oculté</td>\n",
       "      <td>ocultar</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12311</th>\n",
       "      <td>cargarlo</td>\n",
       "      <td>cargar</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12312</th>\n",
       "      <td>enoja</td>\n",
       "      <td>enojar</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12314</th>\n",
       "      <td>rayar</td>\n",
       "      <td>rayar</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12315</th>\n",
       "      <td>parrafada</td>\n",
       "      <td>parrafada</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12317</th>\n",
       "      <td>gordos</td>\n",
       "      <td>gordo</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12318</th>\n",
       "      <td>anónimos</td>\n",
       "      <td>anoì�nimo</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12319</th>\n",
       "      <td>ajetreados</td>\n",
       "      <td>ajetreado</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12320</th>\n",
       "      <td>agradecen</td>\n",
       "      <td>agradecer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12321</th>\n",
       "      <td>bases</td>\n",
       "      <td>base</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12322</th>\n",
       "      <td>recargando</td>\n",
       "      <td>recargar</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12323</th>\n",
       "      <td>mencionado</td>\n",
       "      <td>mencionado</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12324</th>\n",
       "      <td>diseño</td>\n",
       "      <td>disenìƒo</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12325</th>\n",
       "      <td>comprensible</td>\n",
       "      <td>comprensible</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12326</th>\n",
       "      <td>fuer</td>\n",
       "      <td>fuer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12327</th>\n",
       "      <td>finca</td>\n",
       "      <td>finca</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12329</th>\n",
       "      <td>marginada</td>\n",
       "      <td>marginado</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12330</th>\n",
       "      <td>desahogar</td>\n",
       "      <td>desahogar</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12331</th>\n",
       "      <td>felicite</td>\n",
       "      <td>felicitar</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12332</th>\n",
       "      <td>estafo</td>\n",
       "      <td>estafar</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12333</th>\n",
       "      <td>juntaba</td>\n",
       "      <td>juntar</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12334</th>\n",
       "      <td>juntarme</td>\n",
       "      <td>juntar</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12336</th>\n",
       "      <td>relajadamente</td>\n",
       "      <td>relajadamente</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12337</th>\n",
       "      <td>ininterrumpidamente</td>\n",
       "      <td>ininterrumpidamente</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12338</th>\n",
       "      <td>reanudar</td>\n",
       "      <td>reanudar</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12339</th>\n",
       "      <td>ocurría</td>\n",
       "      <td>ocurrir</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12340</th>\n",
       "      <td>vertiginosa</td>\n",
       "      <td>vertiginoso</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12341</th>\n",
       "      <td>advirtió</td>\n",
       "      <td>advertir</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12342</th>\n",
       "      <td>bajaba</td>\n",
       "      <td>bajar</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11361 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Palabra                 Lema  KRS  W2V  FST\n",
       "0                emocional            emocional    0    0    0\n",
       "1                sumamente            sumamente    0    0    0\n",
       "2                inestable            inestable    0    0    0\n",
       "3                   debido               debido    0    0    0\n",
       "4                   estrés             estreì�s    0    0    0\n",
       "5                   supone              suponer    0    0    0\n",
       "6                    falta                falta    0    0    0\n",
       "7                 contacto             contacto    0    0    0\n",
       "8                    hijos                 hijo    0    0    0\n",
       "9               impotencia           impotencia    0    0    0\n",
       "10                  siento               sentar    0    0    0\n",
       "11               constatar            constatar    0    0    0\n",
       "12           especialistas         especialista    0    0    0\n",
       "13                 seguido              seguido    0    0    0\n",
       "14                    caso                 caso    0    0    0\n",
       "15                 querido              querido    0    0    0\n",
       "16                  sabido                saber    0    0    0\n",
       "17                     ver                  ver    0    0    0\n",
       "18            manipulación       manipulacioì�n    0    0    0\n",
       "19                  ejerce              ejercer    0    0    0\n",
       "20                   padre                padre    0    0    0\n",
       "21            preocupación       preocupacioì�n    0    0    0\n",
       "22                análisis           anaì�lisis    0    0    0\n",
       "23                  sangre               sangre    0    0    0\n",
       "25                   altas                 alta    0    0    0\n",
       "26                    hice                hacer    0    0    0\n",
       "27              repetición         repeticioì�n    0    0    0\n",
       "28                 urgente              urgente    0    0    0\n",
       "29               analítica          analiì�tico    0    0    0\n",
       "30                 valores                valor    0    0    0\n",
       "...                    ...                  ...  ...  ...  ...\n",
       "12309            practiqué            practicar    0    0    0\n",
       "12310               oculté              ocultar    0    0    0\n",
       "12311             cargarlo               cargar    0    0    0\n",
       "12312                enoja               enojar    0    0    0\n",
       "12314                rayar                rayar    0    0    0\n",
       "12315            parrafada            parrafada    0    0    0\n",
       "12317               gordos                gordo    0    0    0\n",
       "12318             anónimos            anoì�nimo    0    0    0\n",
       "12319           ajetreados            ajetreado    0    0    0\n",
       "12320            agradecen            agradecer    0    0    0\n",
       "12321                bases                 base    0    0    0\n",
       "12322           recargando             recargar    0    0    0\n",
       "12323           mencionado           mencionado    0    0    0\n",
       "12324               diseño             disenìƒo    0    0    0\n",
       "12325         comprensible         comprensible    0    0    0\n",
       "12326                 fuer                 fuer    0    0    0\n",
       "12327                finca                finca    0    0    0\n",
       "12329            marginada            marginado    0    0    0\n",
       "12330            desahogar            desahogar    0    0    0\n",
       "12331             felicite            felicitar    0    0    0\n",
       "12332               estafo              estafar    0    0    0\n",
       "12333              juntaba               juntar    0    0    0\n",
       "12334             juntarme               juntar    0    0    0\n",
       "12336        relajadamente        relajadamente    0    0    0\n",
       "12337  ininterrumpidamente  ininterrumpidamente    0    0    0\n",
       "12338             reanudar             reanudar    0    0    0\n",
       "12339              ocurría              ocurrir    0    0    0\n",
       "12340          vertiginosa          vertiginoso    0    0    0\n",
       "12341             advirtió             advertir    0    0    0\n",
       "12342               bajaba                bajar    0    0    0\n",
       "\n",
       "[11361 rows x 5 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#edf #data frame donde hay que guardar los embeddings\n",
    "edf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificación de algunos datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape de x_train:\n",
      "(3550,) \n",
      "\n",
      "x_train[0]:\n",
      "igual siempre igual cambia \n",
      "\n",
      "y_train[0]:\n",
      "[1. 0. 0. 0. 0. 0. 0. 1.] \n",
      "\n",
      "x_test[0]:\n",
      "tranquilo salud cambios \n",
      "\n",
      "y_test[0]:\n",
      "[0. 0. 0. 0. 1. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "print('Shape de x_train:')\n",
    "print(x_train.shape,'\\n')\n",
    "\n",
    "print('x_train[0]:')\n",
    "print(x_train[0],'\\n')\n",
    "print('y_train[0]:')\n",
    "print(y_train[0],'\\n')\n",
    "print('x_test[0]:')\n",
    "print(x_test[0],'\\n')\n",
    "print('y_test[0]:')\n",
    "print(y_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo KRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametrización y arquitectura del modelo KRS\n",
    "# Entradas:\n",
    "#  * Datos de entrenamiento: frases de texto\n",
    "#  * Datos de prueba\n",
    "#  * Proporción del vocabulario a utilizar\n",
    "#  * maxima longitud de frases (bool)\n",
    "# Salidas:\n",
    "#  * Datos de entrenamiento y prueba como matrices de números enteros\n",
    "#  * Tokenizador\n",
    "#  * Tamaño del vocabulario\n",
    "#  * Longitud de la secuencia numérica (frase)\n",
    "def keras_embedding(x_train,x_test,voc_prop=1,maxl=True):\n",
    "    # set vocabulary size\n",
    "    train_vocab_size = len(set(word_tokenize(\" \".join(x_train))))\n",
    "    vocab_size = int(voc_prop*train_vocab_size)\n",
    "    print(f\"Hay {train_vocab_size} palabras únicas en todo el set de entrenamiento.\")\n",
    "    print(f\"Conservando {voc_prop*100}% ({vocab_size}) como tamaño del vocabulario.\")\n",
    "\n",
    "    #tokenize train_ and test_ texts and compute integer codes per word\n",
    "    tok = Tokenizer(num_words=vocab_size)\n",
    "    tok.fit_on_texts(x_train)\n",
    "    X_train = tok.texts_to_sequences(x_train)\n",
    "    X_test = tok.texts_to_sequences(x_test)\n",
    "    # Zero padding\n",
    "    lengths = np.array([len(x) for x in X_train])\n",
    "\n",
    "    # Sentence size (input)\n",
    "    if not maxl:\n",
    "        seque_leng = int(lengths.mean() + 2 * lengths.std())\n",
    "        mt = \"(mu + 2*sigma)\"\n",
    "    else:\n",
    "        seque_leng = max(lengths)\n",
    "        mt = \"(max length)\"\n",
    "    print(f\"Longitud de las frases = {seque_leng} palabras {mt}.\")\n",
    "\n",
    "    X_train = sequence.pad_sequences(X_train, maxlen=seque_leng, padding='pre')\n",
    "    X_test = sequence.pad_sequences(X_test, maxlen=seque_leng, padding='pre')\n",
    "\n",
    "    return X_train, X_test,tok,vocab_size,seque_leng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualiza los códigos para X_train[0] <font color='red'>OJO! X_train != x_train</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hay 10289 palabras únicas en todo el set de entrenamiento.\n",
      "Conservando 100% (10289) como tamaño del vocabulario.\n",
      "Longitud de las frases = 77 palabras (max length).\n",
      "\n",
      "Shape de X_train:\n",
      "(3550, 77) \n",
      "\n",
      "\n",
      " X_train[0]:=\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0  62  49  62 548] \n",
      "\n",
      "igual : 62 , siempre : 49 , igual : 62 , cambia : 548 , "
     ]
    }
   ],
   "source": [
    "# X_train != x_train\n",
    "# X_train solo se usa para el modelo KRS\n",
    "X_train,X_test,tokenizer,vs,sl = keras_embedding(x_train,x_test,voc_prop=1)\n",
    "print('\\nShape de X_train:')\n",
    "print(X_train.shape,'\\n')\n",
    "\n",
    "print('\\n X_train[0]:=\\n',X_train[0],'\\n')\n",
    "word_dict=tokenizer.word_index\n",
    "#Check codes\n",
    "for code in X_train[0]:\n",
    "    if code != 0:\n",
    "        print(list(word_dict.keys())[list(word_dict.values()).index(code)],':',code,',',end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arquitectura KRS <font color='red'>(correr antes de un nuevo entrenamiento)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 77, 100)           1028900   \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 77, 32)            9632      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 38, 32)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 264       \n",
      "=================================================================\n",
      "Total params: 1,047,116\n",
      "Trainable params: 1,047,116\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Size of word embeddings\n",
    "vec_dim=100\n",
    "\n",
    "# KRS Model vs y sl calculados arriba\n",
    "KRS = Sequential()\n",
    "KRS.add(Embedding(input_dim=vs, output_dim=vec_dim, input_length=sl))\n",
    "KRS.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "# Param = embedding_vect_len * kernel_size * #filters + #filters = 3104\n",
    "KRS.add(MaxPooling1D(pool_size=2))\n",
    "KRS.add(LSTM(units=32, dropout=0.25, recurrent_dropout=0.25))\n",
    "KRS.add(Dense(units=8, activation='sigmoid'))\n",
    "#KRS.add(Dense(units=8,kernel_regularizer=l2(5E-3), activation='sigmoid'))\n",
    "\n",
    "# compile network\n",
    "KRS.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(KRS.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento del modelo KRS\n",
    "<p>\n",
    "<font color='red'>El modelo se guarda en un archivo que se puede recuperar para extraer los embeddings</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3550 samples, validate on 1522 samples\n",
      "Epoch 1/500\n",
      " - 34s - loss: 0.3950 - acc: 0.8375 - val_loss: 0.4423 - val_acc: 0.8054\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.44231, saving model to KRS_weights_best_3rd.hdf5\n",
      "Epoch 2/500\n",
      " - 34s - loss: 0.2850 - acc: 0.8920 - val_loss: 0.5186 - val_acc: 0.8046\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.44231\n",
      "Epoch 3/500\n",
      " - 34s - loss: 0.2180 - acc: 0.9161 - val_loss: 0.6084 - val_acc: 0.8043\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.44231\n",
      "Epoch 4/500\n",
      " - 34s - loss: 0.1834 - acc: 0.9293 - val_loss: 0.6853 - val_acc: 0.7834\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.44231\n",
      "Epoch 5/500\n",
      " - 34s - loss: 0.1645 - acc: 0.9353 - val_loss: 0.6946 - val_acc: 0.7780\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.44231\n",
      "Epoch 6/500\n",
      " - 34s - loss: 0.1377 - acc: 0.9453 - val_loss: 0.7922 - val_acc: 0.7694\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.44231\n",
      "Epoch 7/500\n",
      " - 34s - loss: 0.1152 - acc: 0.9564 - val_loss: 0.8316 - val_acc: 0.7749\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.44231\n",
      "Epoch 8/500\n",
      " - 34s - loss: 0.0907 - acc: 0.9685 - val_loss: 0.8915 - val_acc: 0.7754\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.44231\n",
      "Epoch 9/500\n",
      " - 37s - loss: 0.0713 - acc: 0.9770 - val_loss: 0.9552 - val_acc: 0.7761\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.44231\n",
      "Epoch 10/500\n",
      " - 36s - loss: 0.0560 - acc: 0.9825 - val_loss: 1.0499 - val_acc: 0.7586\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.44231\n",
      "Epoch 11/500\n",
      " - 33s - loss: 0.0466 - acc: 0.9861 - val_loss: 1.1190 - val_acc: 0.7597\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.44231\n",
      "Epoch 12/500\n",
      " - 34s - loss: 0.0397 - acc: 0.9873 - val_loss: 1.1414 - val_acc: 0.7589\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.44231\n",
      "Epoch 13/500\n",
      " - 33s - loss: 0.0298 - acc: 0.9914 - val_loss: 1.1804 - val_acc: 0.7534\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.44231\n",
      "Epoch 14/500\n",
      " - 33s - loss: 0.0238 - acc: 0.9930 - val_loss: 1.1741 - val_acc: 0.7591\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.44231\n",
      "Epoch 15/500\n",
      " - 33s - loss: 0.0188 - acc: 0.9948 - val_loss: 1.2543 - val_acc: 0.7589\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.44231\n",
      "Epoch 16/500\n",
      " - 33s - loss: 0.0160 - acc: 0.9957 - val_loss: 1.2956 - val_acc: 0.7576\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.44231\n",
      "Epoch 17/500\n",
      " - 34s - loss: 0.0134 - acc: 0.9961 - val_loss: 1.3210 - val_acc: 0.7570\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.44231\n",
      "Epoch 18/500\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[224,77,300] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node training_1/Adam/gradients/conv1d_2/convolution/Conv2D_grad/Conv2DBackpropInput}} = Conv2DBackpropInput[T=DT_FLOAT, _class=[\"loc:@train...propFilter\"], data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](training_1/Adam/gradients/conv1d_2/convolution/Conv2D_grad/ShapeN, conv1d_2/convolution/ExpandDims_1, training_1/Adam/gradients/conv1d_2/convolution/Squeeze_grad/Reshape)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-a9e908c3192b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mcallbacks_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stop\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mKRS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepocas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\valiant\\anaconda3\\envs\\tensorfl\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mc:\\users\\valiant\\anaconda3\\envs\\tensorfl\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\valiant\\anaconda3\\envs\\tensorfl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\valiant\\anaconda3\\envs\\tensorfl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\valiant\\anaconda3\\envs\\tensorfl\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\valiant\\anaconda3\\envs\\tensorfl\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    526\u001b[0m             \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    529\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[1;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[224,77,300] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node training_1/Adam/gradients/conv1d_2/convolution/Conv2D_grad/Conv2DBackpropInput}} = Conv2DBackpropInput[T=DT_FLOAT, _class=[\"loc:@train...propFilter\"], data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](training_1/Adam/gradients/conv1d_2/convolution/Conv2D_grad/ShapeN, conv1d_2/convolution/ExpandDims_1, training_1/Adam/gradients/conv1d_2/convolution/Squeeze_grad/Reshape)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "\n",
    "filepath=\"KRS_weights_best_3rd.hdf5\" #último archivo guardado\n",
    "epocas=500\n",
    "paciencia=20\n",
    "batch=4\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=paciencia, mode='auto') \n",
    "callbacks_list = [checkpoint, early_stop]\n",
    "\n",
    "KRS.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=batch, verbose=2, epochs=epocas, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------\n",
    "<h1><center><font color='blue'>POR HACER A PARTIR DE AQUI:</font></center></h1> \n",
    "# --------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <p>\n",
    " <ol>\n",
    "  <li>Recuperar los vectores de KRS por cada palabra y almacenarlos en <b>edf</b>:\n",
    "      <ul>\n",
    "          <li>Checar el archivo WRD_EMBED_TUT en la parte de <b>Extracción de WE</b></li>\n",
    "      </ul>\n",
    "  </li>\n",
    "  <li>Terminar de implementar el <b>Modelo Word2Vec</b>. Checar estas referencias:</li>\n",
    "      <ul>\n",
    "          <li><a href=\"https://rare-technologies.com/word2vec-tutorial/\">Tutorial W2V</a></li>\n",
    "          <li><a href=\"https://www.shanelynn.ie/word-embeddings-in-python-with-spacy-and-gensim/\">WE in Python with gensim</a></li>\n",
    "          <li><a href=\"https://radimrehurek.com/gensim/models/keyedvectors.html\">gensim models.keyedvectors – Store and query word vectors</a></li>\n",
    "      </ul>\n",
    "  <li>Implementar el <b>Modelo FastText</b></li>\n",
    "      <ul>\n",
    "          <li><a href=\"https://radimrehurek.com/gensim/models/fasttext.html#module-gensim.models.fasttext\">gensim models.fasttext – FastText model</a></li>\n",
    "          <li><a href=\"https://rare-technologies.com/fasttext-and-gensim-word-embeddings/\">Discusión FastText model</a></li>\n",
    "          <li><a href=\"https://pypi.org/project/fasttext/\">ALTERNATIVAMENTE se puede usar este Módulo Fast-Text de Python (no es gensim)</a></li>\n",
    "      </ul>\n",
    "</ol>\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10288\n",
      "[ 0.05839317  0.0607519   0.06340941  0.11794056  0.03106063 -0.05368104\n",
      "  0.10091178  0.01456912  0.14017285 -0.09613916 -0.22649904  0.07540453\n",
      " -0.01763227 -0.11013029  0.13828623 -0.09858195  0.13289987 -0.03413352\n",
      "  0.02314154  0.05519608  0.1045882  -0.05120502  0.0125187  -0.02229746\n",
      " -0.04782974  0.03385207 -0.00480956  0.06128099 -0.04408147 -0.0176931\n",
      "  0.04729763  0.12209062  0.04550756  0.0948991   0.03669843 -0.03603292\n",
      "  0.05135896  0.04322938  0.02998595 -0.03624516 -0.01675601 -0.00812088\n",
      "  0.18575849  0.12628679 -0.02373976  0.09924906 -0.05491759 -0.0135337\n",
      " -0.11440403  0.08987962  0.05538515  0.21145648 -0.04873279 -0.06155011\n",
      "  0.00355819 -0.07464249 -0.03159312 -0.05774809  0.16468732  0.04846052\n",
      " -0.00711633  0.02103161  0.08983466  0.00685691  0.03039785 -0.07053053\n",
      "  0.02262693 -0.05627981 -0.01505407 -0.10534882  0.08005828  0.16184339\n",
      " -0.1380147   0.09245729 -0.02949142  0.17380197  0.05781288 -0.02305831\n",
      "  0.00528593  0.0676351  -0.03679289  0.05717481  0.12062025 -0.00155153\n",
      " -0.03681147 -0.07150339  0.03400176 -0.1005208  -0.09782514  0.0864966\n",
      " -0.04079489 -0.06694159 -0.03767401  0.03917642 -0.21730514 -0.12574826\n",
      " -0.14427932  0.14587297 -0.09897065  0.01511614]\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "vec1 = KRS.layers[0].get_weights()[0]\n",
    "KRSvec=[]\n",
    "\n",
    "for key in tokenizer.word_index.keys():    \n",
    "    code = tokenizer.word_index[key]       \n",
    "    if(code<vec1.shape[0]):\n",
    "        emb1 = vec1[code]               \n",
    "        KRSvec.append(emb1)\n",
    "\n",
    "print(len(KRSvec))\n",
    "print(KRSvec[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import modules & set up logging\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Preliminary KNOWLEDGE:\n",
    "# gensim’s word2vec expects a sequence of sentences as its input. \n",
    "# Each sentence a list of words (utf8 strings): \n",
    "# FOR INSTANCE:\n",
    "# sentences = [['first', 'sentence'], ['second', 'sentence']]\n",
    "# train word2vec on the two sentences\n",
    "# model = gensim.models.Word2Vec(sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterador (ahorra memoria) sobre documentos que contienen una frase de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class MySentences(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    "    # Iterador de frases\n",
    "    # El iterador abre uno a uno los archivos que contienen cada una de las frases\n",
    "    # Por lo tanto debe haber 3550 documentos, \n",
    "    # que corresponden a las instancias en x_train\n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            for line in open(os.path.join(self.dirname, fname)):\n",
    "                yield line.split()\n",
    "# La carpeta Frases está VACÍA!! \n",
    "# Necesitamos llenarla con documentos que contengan las instancias de x_train\n",
    "# Así, la variable all_sentences contiene las instancias de entrenamiento\n",
    "all_sentences = MySentences('Frases/') # a memory-friendly iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento de Word2Vec\n",
    "<p>\n",
    "Se puede jugar con los parámetros\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(all_sentences, \n",
    "                 min_count=1,   # Ignore words that appear less than this\n",
    "                 size=100,      # Dimensionality of word embeddings\n",
    "                 workers=4,     # Number of processors (parallelisation) requires cython\n",
    "                 window=5,      # Context window for words during training\n",
    "                 iter=30)       # Number of epochs training over corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# para convertir esta libreta en un programa .py que puede ser llamado\n",
    "# NO ES FORZOSO CORRER ESTA INSTRUCCION\n",
    "!jupyter nbconvert --to script SMS_Embeddings.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
